# export AWS_DEFAULT_REGION='us-west-2'
# nohup streamlit run app.py --server.port 8503 &
# ssh -i /Users/chiholee/Desktop/Project/keys/summit2024-key.pem -L 13306:summit2024.cluster-cdoccmmce0bj.ap-northeast-2.rds.amazonaws.com:3306 ec2-user@52.79.232.79

import streamlit as st
import fitz
import logging
import boto3
import json
import os
import re
import pymysql
import pandas as pd
from dotenv import load_dotenv
from datetime import datetime
import time
from langchain_community.chat_models import BedrockChat
from langchain_community.embeddings import BedrockEmbeddings
from langchain_community.vectorstores import OpenSearchVectorSearch
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms.bedrock import Bedrock
from opensearchpy import OpenSearch, RequestsHttpConnection
from opensearchpy.helpers import bulk
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any

from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory
from langchain.memory import ConversationBufferMemory

identity_id = 'test030'

def get_memory_from_dynamo(session_id):
  chat_history = DynamoDBChatMessageHistory(table_name="memories-dev", session_id=session_id)

  print("# message_history : ", chat_history)
  
  return ConversationBufferMemory(
    memory_key="chat_history", 
    chat_memory=chat_history, 
    return_messages=True,
    ai_prefix="AI",
    human_prefix="Human"
  ), chat_history

class StreamHandler(BaseCallbackHandler):
    def __init__(self, initial_text=""):
        self.container = st.empty()
        self.initial_text = initial_text
        self.text = initial_text

    
    def on_llm_start(self, *args: Any, **kwargs: Any):
        self.text = self.initial_text
        # # Weird code. But just works fine.
        # with st.chat_message("assistant"):
        #     self.container = st.empty()

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        # Add to UI Only
        self.text += token
        self.container.markdown(self.text, unsafe_allow_html=True)
        # print(token, end="")

    def on_llm_end(self, *args: Any, **kwargs: Any) -> None:
        # Add to state
        st.session_state.messages.append({
            "role": "assistant",
            "type": "text",
            "content": self.text
        })
        


logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# - <font color='#32CD32;'><b>ì–´ì œ íŒë§¤ëœ ìƒí’ˆ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë¬¸ ê¸ˆì•¡ TOP 5 ë¥¼ ì•Œë ¤ì¤˜</b></font><br>
# - <font color='#32CD32;'><b>ì§€ë‚œ ì¼ì£¼ì¼ê°„ ì£¼ë¬¸ ì‹¤ì ì„ ì¼ ë³„ë¡œ ì•Œë ¤ì¤˜</b></font><br>
# - <font color='#32CD32;'><b>ìµœê·¼ 5ë¶„ ë™ì•ˆ ì´ì£¼ë¬¸ê¸ˆì•¡ê³¼ ì´ì£¼ë¬¸ìˆ˜ëŸ‰ì„ ë¶„ ë‹¨ìœ„ë¡œ ì•Œë ¤ì¤˜</b></font><br>
# - <font color='#32CD32;'><b>ì˜¤ëŠ˜ ì´ ì£¼ë¬¸ê¸ˆì•¡ì´ ê°€ì¥ ì ì€ ìƒí’ˆì„ ì•Œë ¤ì¤˜</b></font><br>

INIT_MESSAGE = {"role": "assistant",
                "type": "text",
                "content": """
ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” <font color='red'><b>Amazon Bedrockê³¼ Claude3</b></font>ë¥¼ í™œìš©í•´ì„œ ì—¬ëŸ¬ë¶„ë“¤ì´ ì°¾ê³  ì‹¶ì€ ë°ì´í„°ë¥¼ ëŒ€ì‹  ì°¾ì•„ì¤„ <i><b>[ë°ì´í„°ê°€ ê¶ê¸ˆí•´]<i><b> ì…ë‹ˆë‹¤. 
<br>ì•„ë˜ì™€ ê°™ì´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.
- <font color='#32CD32;'><b>ì£¼ë¬¸ ì „í™˜ìœ¨ì— ëŒ€í•´ ì„¤ëª…í•´ì¤„ë˜?</b></font><br>
- <font color='#32CD32;'><b>ìµœê·¼ 5ë¶„ ê°„ ìƒí’ˆ ë³„ ì£¼ë¬¸ì „í™˜ìœ¨ top 5 ë°ì´í„°ë¥¼ ì•Œë ¤ì¤˜</b></font><br>
- <font color='#32CD32;'><b>ì´ë²¤íŠ¸ë¥¼ ë‹¤ì‹œ í–ˆë‹¤. ìµœê·¼ 5ë¶„ ê°„ ìƒí’ˆ ë³„ ì£¼ë¬¸ì „í™˜ìœ¨ top 5 ë°ì´í„°ë¥¼ ì•Œë ¤ì¤˜</b></font><br>
---
ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"""}

select_options = ['ìš©ì–´ê°€ ê¶ê¸ˆí•´', 'ë°ì´í„°ê°€ ê¶ê¸ˆí•´']



################################################################################

load_dotenv()
opensearch_username = os.getenv('OPENSEARCH_USERNAME')
opensearch_password = os.getenv('OPENSEARCH_PASSWORD')
opensearch_endpoint = os.getenv('OPENSEARCH_ENDPOINT')
index_name = os.getenv('OPENSEARCH_INDEX_NAME')
mysql_host = os.getenv('MYSQL_HOST')
mysql_port = os.getenv('MYSQL_PORT')
mysql_user = os.getenv('MYSQL_USER')
mysql_password = os.getenv('MYSQL_PASSWORD')
mysql_db = os.getenv('MYSQL_DB')

bedrock_region = 'us-west-2'
stop_record_count = 100
record_stop_yn = False
bedrock_model_id = "anthropic.claude-3-sonnet-20240229-v1:0"
bedrock_embedding_model_id = "amazon.titan-embed-text-v1"
################################################################################

def get_athena_client() :
    athena_client = boto3.client('athena',
                       region_name='ap-northeast-2')
    return athena_client


def get_opensearch_cluster_client():
    opensearch_client = OpenSearch(
        hosts=[{
            'host': opensearch_endpoint,
            'port': 443
        }],
        http_auth=(opensearch_username, opensearch_password),
        index_name=index_name,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
        timeout=30
    )
    return opensearch_client


def get_bedrock_client():
    bedrock_client = boto3.client(
        "bedrock-runtime", region_name=bedrock_region)
    return bedrock_client


def create_langchain_vector_embedding_using_bedrock(bedrock_client):
    bedrock_embeddings_client = BedrockEmbeddings(
        client=bedrock_client,
        model_id=bedrock_embedding_model_id)
    return bedrock_embeddings_client


def create_opensearch_vector_search_client(bedrock_embeddings_client, _is_aoss=False):
    docsearch = OpenSearchVectorSearch(
        index_name=index_name,
        embedding_function=bedrock_embeddings_client,
        opensearch_url=f"https://{opensearch_endpoint}",
        http_auth=(opensearch_username, opensearch_password),
        is_aoss=_is_aoss
    )
    return docsearch


def create_bedrock_llm():
    # claude-2 ì´í•˜
    # bedrock_llm = Bedrock(
    #     model_id=model_version_id,
    #     client=bedrock_client,
    #     model_kwargs={'temperature': 0}
    #     )
    # bedrock_llm = BedrockChat(model_id=model_version_id, model_kwargs={'temperature': 0}, streaming=True)

    bedrock_llm = BedrockChat(
        model_id=bedrock_model_id, 
        model_kwargs={'temperature': 0},
        streaming=True,
        callbacks=[StreamHandler()]
        )
    return bedrock_llm


def get_bedrock_client():
    bedrock_client = boto3.client(
        "bedrock-runtime", region_name=bedrock_region)
    return bedrock_client


def create_vector_embedding_with_bedrock(text, bedrock_client):
    payload = {"inputText": f"{text}"}
    body = json.dumps(payload)
    modelId = "amazon.titan-embed-text-v1"
    accept = "application/json"
    contentType = "application/json"

    response = bedrock_client.invoke_model(
        body=body, modelId=modelId, accept=accept, contentType=contentType
    )
    response_body = json.loads(response.get("body").read())

    embedding = response_body.get("embedding")
    return {"_index": index_name, "text": text, "vector_field": embedding}

def create_opensearch_index(opensearch_client) :
    body = {
        'settings': {
            'index': {
                'number_of_shards': 3,
                'number_of_replicas': 2,
                "knn": True,
                "knn.space_type": "cosinesimil"
            }
        }
    }
    success = opensearch_client.indices.create(index_name, body=body)
    if success:
        body = {
            "properties": {
                "vector_field": {
                    "type": "knn_vector",
                    "dimension": 1536
                },
                "text": {
                    "type": "keyword"
                }
            }
        }
        success = opensearch_client.indices.put_mapping(
            index=index_name,
            body=body
        )


def extract_sentences_from_pdf(opensearch_client, pdf_file, progress_bar, progress_text):
    try:
        logging.info(
            f"Checking if index {index_name} exists in OpenSearch cluster")

        exists = opensearch_client.indices.exists(index=index_name)

        if not exists:
            create_opensearch_index(opensearch_client)

        doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
        all_records = []
        for page in doc:
            all_records.append(page.get_text())

        logging.info(f"PDF LIST ê°œìˆ˜ : {len(all_records)}")

        total_records = len(all_records)
        processed_records = 0

        bedrock_client = get_bedrock_client()

        all_json_records = []

        for record in all_records:
            if record_stop_yn and processed_records > stop_record_count:

                success, failed = bulk(opensearch_client, all_json_records)
                break

            records_with_embedding = create_vector_embedding_with_bedrock(
                record, bedrock_client)
            all_json_records.append(records_with_embedding)

            processed_records += 1
            progress = int((processed_records / total_records) * 100)
            progress_bar.progress(progress)

            if processed_records % 500 == 0 or processed_records == len(all_records):

                success, failed = bulk(opensearch_client, all_json_records)
                all_json_records = []

        progress_text.text("ì™„ë£Œ")
        logging.info("ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì½”ë“œ ìƒì„± ì™„ë£Œ")

        return total_records
    except Exception as e:
        print(str(e))
        st.error('PDFë¥¼ ì„ë² ë”© í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒë˜ì—ˆìŠµë‹ˆë‹¤.')
        return 0

def analytics_in_data(question, data):
    question = question
    bedrock_client = get_bedrock_client()
    bedrock_llm = create_bedrock_llm()

    bedrock_embeddings_client = create_langchain_vector_embedding_using_bedrock(
        bedrock_client)

    opensearch_vector_search_client = create_opensearch_vector_search_client(
        bedrock_embeddings_client)
    
    prompt_template = """
    Use the following pieces of context to answer the question at the end.
    ì§ˆë¬¸ì„ í†µí•´ ì–»ì€ ë°ì´í„°ì´ë‹¤. 
    ë°ì´í„°ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œí•´ì¤˜.
    ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì¤˜. 
    ì¸ì‚¬ì´íŠ¸ ë‹¨ì–´ëŠ” ì´ˆë¡ìƒ‰ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì¸ì‚¬ì´íŠ¸ ë‹¨ì–´ ì•ì— :thinking_face: ë¥¼ ì ì–´ì¤˜.
    ë°ì´í„°ì˜ ì˜ë¯¸ ë¶€ë¶„ì—ì„œ ìˆ«ìëŠ” ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œí•´ì¤˜. ìƒ‰ê¹”ì„ ìœ„í•œ íƒœê·¸ëŠ” span ì„ ì‚¬ìš©í•´.
    ê¸ˆì•¡ì€ 1000ìë¦¬ ë§ˆë‹¤ ì½¤ë§ˆë¥¼ í‘œì‹œí•´ì¤˜.
    ê³¼ê±° ëŒ€í™” ì´ë ¥ì˜ ë°ì´í„°ì™€ ì°¨ì´ê°€ ìˆë‹¤ë©´ ì°¨ì´ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í•¨ê»˜ ì•Œë ¤ì¤˜. ì˜ˆë¥¼ ë“¤ë©´ 1ìœ„ ìƒí’ˆì˜ ë³€í™”ê°€ ìˆì—ˆëŠ”ì§€ ë“±.
    context ì •ë³´ëŠ” ë¬´ì‹œí•´.
    {context}
    
    # ì§ˆë¬¸
    {question}

    ë‹¤ìŒ ëŒ€í™”ì™€ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜.
    %s

    # ë°ì´í„°
    %s

    
    # Answer
    ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.(í…Œì´ë¸” í˜•íƒœì˜ ë°ì´í„°, í…Œì´ë¸” ì»¬ëŸ¼ì€ í•œê¸€ì„ ì‚¬ìš©)
    ...

    ë°ì´í„°ì˜ ì˜ë¯¸ì™€ ì–»ì„ ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 
    - ...
    - ...
    """


    data = data.replace("{","")
    data = data.replace("}","")

    memory, chat_history = get_memory_from_dynamo(identity_id)
    
    prompt_template = PromptTemplate(
            template=prompt_template % (chat_history, data), 
            input_variables=["context", "chat_history", "question"]
        )

    # qa = RetrievalQA.from_chain_type(llm=bedrock_llm,
    #                                      chain_type="stuff",
    #                                      retriever=opensearch_vector_search_client.as_retriever(),
    #                                      return_source_documents=True,
    #                                      chain_type_kwargs={
    #                                          "prompt": prompt_template})

    qa = RetrievalQA.from_chain_type(llm=bedrock_llm,
                                        chain_type="stuff",
                                        retriever=opensearch_vector_search_client.as_retriever(),
                                        # return_source_documents=True,
                                        return_source_documents=False,
                                        memory= memory,
                                        chain_type_kwargs={
                                            "prompt": prompt_template})



    response = qa(question,
                      return_only_outputs=False)
    
    return f"{response.get('result')}"


def find_answer_in_sentences(select_option, question):
    # try:
    print("# select_option : ", select_option)
    question = question
    bedrock_client = get_bedrock_client()
    bedrock_llm = create_bedrock_llm()

    bedrock_embeddings_client = create_langchain_vector_embedding_using_bedrock(
        bedrock_client)

    opensearch_vector_search_client = create_opensearch_vector_search_client(
        bedrock_embeddings_client)
    
    
    prompt_template = {
        # ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´
        # Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. don't include harmful content
        # 0 : """
        #     You are a Bioinformatics expert with immense knowledge and experience in the field.
        #     Answer my questions based on your knowledge and our older conversation. Do not make up answers.
        #     If you do not know the answer to a question, just say "I don't know".

        #     {context}

        #     Given the following conversation and a follow up question, answer the question.

        #     %s

        #     question: {question}
        #     """,
            # ë„ˆëŠ” AíšŒì‚¬ì—ì„œ ê·¼ë¬´í•˜ëŠ” ì§ì›ë“¤ì—ê²Œ ë‚´ë¶€ ìš©ì–´, ê³„ì‚°ì‹, ê·¸ë¦¬ê³  ë°ì´í„° ì¡°íšŒë¥¼ ë„ì™€ì£¼ëŠ” "ë°ì´í„°ê°€ ê¶ê¸ˆí•´" ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ì±—ë´‡ì´ë‹¤. 
            # contextì— ì œê³µëœ ë‚´ìš©ì´ ì—†ë‹¤ë©´ ë°˜ë“œì‹œ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•´ì¤˜.
            # contextì— ì œê³µëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì§ˆë¬¸í•œ ê²ƒì— ëŒ€í•œ ì •ì˜ì™€ ê³„ì‚°ì‹ ê·¸ë¦¬ê³  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ ëŒ€ë‹µí•´ì¤˜. SQLì€ ì‘ì„±í•˜ì§€ë§ˆ.
        0 : """
            ë„ˆëŠ” AíšŒì‚¬ì—ì„œ ê·¼ë¬´í•˜ëŠ” ì§ì›ë“¤ì—ê²Œ ë‚´ë¶€ ìš©ì–´, ê³„ì‚°ì‹, ê·¸ë¦¬ê³  ë°ì´í„° ì¡°íšŒë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì´ë‹¤. 
            contextì— ì œê³µëœ ë‚´ìš©ì´ ì—†ë‹¤ë©´ ë°˜ë“œì‹œ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•´ì¤˜.
            contextì— ì œê³µëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì§ˆë¬¸í•œ ê²ƒì— ëŒ€í•œ ì •ì˜ì™€ ê³„ì‚°ì‹ ê·¸ë¦¬ê³  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ ëŒ€ë‹µí•´ì¤˜. SQLì€ ì‘ì„±í•˜ì§€ë§ˆ.
            ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ëŠ” ë¶‰ì€ìƒ‰ìœ¼ë¡œ í‘œì‹œí•´ì¤˜.
            {context}

            ë‹¤ìŒ ëŒ€í™”ì™€ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜.
            %s


            * Question: {question}
            * Answer:
            """,
        # ë°ì´í„° ì¡°íšŒ
            # ê³¼ê±° ëŒ€í™” ì´ë ¥ì— ì œê³µë˜ì—ˆë˜ SQL ì´ë¼ë©´ SQLì„ MARKDOWN ì˜ TOGGLE í˜•íƒœì˜ í¬ë§·ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
            # ë°ì´í„° ìš”ì²­ì„ í•œë‹¤ë©´ SQLë¥¼ ì œê³µí•´ì¤˜.
            # ë‹¤ìŒ ëŒ€í™”ì™€ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜.
            # %s
        1 : """
            ë„ˆëŠ” AíšŒì‚¬ì—ì„œ ê·¼ë¬´í•˜ëŠ” ì§ì›ë“¤ì—ê²Œ ë‚´ë¶€ ìš©ì–´, ê³„ì‚°ì‹, ê·¸ë¦¬ê³  ë°ì´í„° ì¡°íšŒë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì´ë‹¤. 
            ë°ì´í„° ìš”ì²­ì„ í•˜ë©´ contextì— ì •í™•í•œ ë°ì´í„°ë² ì´ìŠ¤ëª…, í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…ì´ ëª¨ë‘ ì—†ì„ ê²½ìš°ì—ëŠ” ì˜ˆì¸¡í•´ì„œ SQLì„ ì‘ì„±í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•´ì¤˜.
            ë°ì´í„° ìš”ì²­ì„ í•˜ë©´ contextì— ì •í™•í•œ í…Œì´ë¸”ëª…ê³¼ ì»¬ëŸ¼ëª…ì´ ëª¨ë‘ ìˆì„ ê²½ìš°ì—ë§Œ AWS ATHENAì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ SQL ì„ MARKDOWN ì½”ë“œì˜ SQLíƒœê·¸ ì•ˆì— ì‘ì„±í•´ì¤˜, SQL ì€ <details open><summary>SQLì œëª©</summary>```sql\nSQL```</details> í¬ë§·ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
            {context}


            * Question: {question}
            * Answer:
            """
    }
        
    print("## prompt : ", prompt_template[select_option])
    
    # memory, chat_history = get_memory_from_dynamo(identity_id)

    prompt = PromptTemplate(
        # template=prompt_template[select_option] % (chat_history), input_variables=["context", "chat_history", "question"]
        template=prompt_template[select_option], input_variables=["context", "question"]
    )


    qa = RetrievalQA.from_chain_type(llm=bedrock_llm,
                                        chain_type="stuff",
                                        retriever=opensearch_vector_search_client.as_retriever(),
                                        # return_source_documents=True,
                                        return_source_documents=False,
                                        chain_type_kwargs={
                                            "prompt": prompt})
    # response = qa(question,
    #                 return_only_outputs=False)
    
    response = qa(question,
                    return_only_outputs=False)

    return f"{response.get('result')}"
    # except Exception as e:
    #     if 'index_not_found_exception' in str(e):
    #         st.error('ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—…ë¡œë“œ í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”')
    #     else:
    #         print(str(e))
    #         # st.error('ë‹µë³€ì„ ì°¾ëŠ” ê³¼ì •ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    #         st.error(str(e))
    #     return "ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def connect_to_database():
    return pymysql.connect(
        host=mysql_host,
        port=int(mysql_port),
        user=mysql_user,
        password=mysql_password,
        database=mysql_db,
        charset='utf8mb4'
    )

def execute_query_and_return_df(sql):
    conn = connect_to_database()
    try:
        with conn.cursor() as cursor:
            cursor.execute(sql)
            result = cursor.fetchall()
            df = pd.DataFrame(result, columns=[i[0]
                              for i in cursor.description])
    finally:
        conn.close()
    return df


def execute_query_athena(client, sql):
    
    s3_client = boto3.client('s3', region_name='ap-northeast-2')
    
    s3_output = 's3://korea-summit2024/tmp/athena'
    response = client.start_query_execution(
        QueryString=sql,
        ResultConfiguration={
            'OutputLocation': s3_output,
        }
    )
    query_execution_id = response['QueryExecutionId']

    while True:
        query_status = client.get_query_execution(QueryExecutionId=query_execution_id)
        query_execution_status = query_status['QueryExecution']['Status']['State']
        
        if query_execution_status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
            break
        else:
            time.sleep(2)
    
    if query_execution_status == 'SUCCEEDED':
        result_location = query_status['QueryExecution']['ResultConfiguration']['OutputLocation']
        filename = result_location.split('/')[-1]
        s3_path = f's3://{s3_output}/{filename}'
        
        df = pd.read_csv(s3_path)
        json_str = df.to_json(orient='records', lines=True)

        return json_str

    



def main():

    
    opensearch_client = get_opensearch_cluster_client()
    st.set_page_config(page_title='ğŸ¤– Chat with Bedrock', layout='wide')
    # st.header('_Chatbot_ using :blue[OpenSearch] :sunglasses:', divider='rainbow')
    st.header(':blue[ë°ì´í„°ê°€] _ê¶ê¸ˆí•´_ :sunglasses:', divider='rainbow')    

    if 'qa_history' not in st.session_state:
        st.session_state.qa_history = []

    with st.sidebar:
        
        st.sidebar.markdown(
            ':smile: **Createby:** chiholee@amazon.com', unsafe_allow_html=True)
        st.sidebar.markdown('---')
        selected_option = st.sidebar.selectbox(
            'ì–´ë–¤ ì˜µì…˜ì„ ì„ íƒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?',
            select_options
        )
        selected_index = select_options.index(selected_option)
        st.session_state.select_option = selected_index
        # st.write(st.session_state.select_option)
        st.sidebar.markdown('---')
        st.title("RAG Embedding")
        pdf_file = st.file_uploader(
            "PDF ì—…ë¡œë“œë¥¼ í†µí•´ ì¶”ê°€ í•™ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", type=["pdf"], key=None)
        

        if 'last_uploaded' not in st.session_state:
            st.session_state.last_uploaded = None

        if pdf_file is not None and pdf_file != st.session_state.last_uploaded:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
            # opensearch_client.indices.delete(index=index_name)

            progress_text = st.empty()
            st.session_state['progress_bar'] = st.progress(0)
            progress_text.text("RAG(OpenSearch) ì„ë² ë”© ì¤‘...")
            record_cnt = extract_sentences_from_pdf(
                opensearch_client, pdf_file, st.session_state['progress_bar'], progress_text)
            if record_cnt > 0:
                st.session_state['processed'] = True
                st.session_state['record_cnt'] = record_cnt
                st.session_state['progress_bar'].progress(100)
                st.session_state.last_uploaded = pdf_file
                st.success(f"{record_cnt} Vector ì„ë² ë”© ì™„ë£Œ!")
        
        if st.button("ê¸°ì¡´ ì—…ë¡œë“œ ë¬¸ì„œ ì‚­ì œ"):
            exists = opensearch_client.indices.exists(index=index_name)

            if exists:
                opensearch_client.indices.delete(index=index_name)
                create_opensearch_index(opensearch_client)            
                logging.info("OpenSearch index successfully deleted")
                st.success("OpenSearch ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")


    if "messages" not in st.session_state.keys():
        st.session_state.messages = [INIT_MESSAGE]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["type"] == "text":
                st.markdown(message["content"], unsafe_allow_html=True)

    question = st.chat_input("Say something")

    if question:
        st.session_state.messages.append({"role": "user",
                                          "type": "text",
                                          "content": question})
        with st.chat_message("user"):
            st.markdown(question, unsafe_allow_html=True)

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                answer = find_answer_in_sentences(st.session_state.select_option, question)

            with st.spinner("ë°ì´í„° ì¡°íšŒ ì¤‘..."):

                try :
                    sql_queries = re.findall(r'```sql(.*?)```', answer, re.DOTALL)

                    if len(sql_queries) > 0:
                        sql = sql_queries[0]
                        df = execute_query_athena(get_athena_client(), sql)
                        analytics_in_data(question, df)
                except Exception as e:                    
                    # st.error("SQL ìˆ˜í–‰ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    print("# ì—ëŸ¬", str(e))
                    pass


if __name__ == "__main__":
    main()
