import streamlit as st
import fitz
import logging
import boto3
import json
import os
import re
import pymysql
import pandas as pd
from dotenv import load_dotenv
from datetime import datetime
import time
from langchain_community.chat_models import BedrockChat
from langchain_community.embeddings import BedrockEmbeddings
from langchain_community.vectorstores import OpenSearchVectorSearch
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms.bedrock import Bedrock
from opensearchpy import OpenSearch, RequestsHttpConnection
from opensearchpy.helpers import bulk
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any

from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory
from langchain.memory import ConversationBufferMemory

identity_id = 'SESSION-ID-02'

def get_memory_from_dynamo(session_id):
  chat_history = DynamoDBChatMessageHistory(table_name="memories-dev", session_id=session_id)

  print("# message_history : ", chat_history)
  
  return ConversationBufferMemory(
    memory_key="chat_history", 
    chat_memory=chat_history, 
    return_messages=True,
    ai_prefix="AI",
    human_prefix="Human"
  ), chat_history

class StreamHandler(BaseCallbackHandler):
    def __init__(self, initial_text=""):
        self.container = st.empty()
        self.initial_text = initial_text
        self.text = initial_text

    
    def on_llm_start(self, *args: Any, **kwargs: Any):
        self.text = self.initial_text

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self.text += token
        self.container.markdown(self.text, unsafe_allow_html=True)

    def on_llm_end(self, *args: Any, **kwargs: Any) -> None:
        st.session_state.messages.append({
            "role": "assistant",
            "type": "text",
            "content": self.text
        })

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

INIT_MESSAGE = {"role": "assistant",
                "type": "text",
                "content": """
ì €ëŠ” <font color='red'><b>Amazon OpenSearch</b></font> ì™€ <font color='red'><b>Amazon Bedrock</b></font>ë¥¼ í™œìš©í•´ì„œ ë°ì´í„°ë¥¼ ëŒ€ì‹  ì°¾ì•„ì¤„ <font color='red'><b>Bot</b></font> ì…ë‹ˆë‹¤. 
<br>ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”.
- <font color='#32CD32;'><b>ìš°ë¦¬íšŒì‚¬ì˜ ì£¼ë¬¸ì „í™˜ìœ¨ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜</b></font><br>
- <font color='#32CD32;'><b>ìš°ë¦¬íšŒì‚¬ì˜ ìµœê·¼ 1ì‹œê°„ ë™ì•ˆ ìƒí’ˆ ë³„ ì£¼ë¬¸ì „í™˜ìœ¨ top 5 ë°ì´í„°ë¥¼ ì•Œë ¤ì¤˜</b></font><br>
---
ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"""}

select_options = ['ìš©ì–´ê°€ ê¶ê¸ˆí•´', 'ë°ì´í„°ê°€ ê¶ê¸ˆí•´']


load_dotenv()
opensearch_username = os.getenv('OPENSEARCH_USERNAME')
opensearch_password = os.getenv('OPENSEARCH_PASSWORD')
opensearch_endpoint = os.getenv('OPENSEARCH_ENDPOINT')
index_name = os.getenv('OPENSEARCH_INDEX_NAME')
mysql_host = os.getenv('MYSQL_HOST')
mysql_port = os.getenv('MYSQL_PORT')
mysql_user = os.getenv('MYSQL_USER')
mysql_password = os.getenv('MYSQL_PASSWORD')
mysql_db = os.getenv('MYSQL_DB')

bedrock_region = 'us-west-2'
stop_record_count = 100
record_stop_yn = False
bedrock_model_id = "anthropic.claude-3-sonnet-20240229-v1:0"
bedrock_embedding_model_id = "amazon.titan-embed-text-v1"

def get_athena_client() :
    athena_client = boto3.client('athena',
                       region_name='ap-northeast-2')
    return athena_client


def get_opensearch_cluster_client():
    opensearch_client = OpenSearch(
        hosts=[{
            'host': opensearch_endpoint,
            'port': 443
        }],
        http_auth=(opensearch_username, opensearch_password),
        index_name=index_name,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
        timeout=30
    )
    return opensearch_client


def get_bedrock_client():
    bedrock_client = boto3.client(
        "bedrock-runtime", region_name=bedrock_region)
    return bedrock_client


def create_langchain_vector_embedding_using_bedrock(bedrock_client):
    bedrock_embeddings_client = BedrockEmbeddings(
        client=bedrock_client,
        model_id=bedrock_embedding_model_id)
    return bedrock_embeddings_client


def create_opensearch_vector_search_client(bedrock_embeddings_client, _is_aoss=False):
    docsearch = OpenSearchVectorSearch(
        index_name=index_name,
        embedding_function=bedrock_embeddings_client,
        opensearch_url=f"https://{opensearch_endpoint}",
        http_auth=(opensearch_username, opensearch_password),
        is_aoss=_is_aoss
    )
    return docsearch


def create_bedrock_llm():
    # claude-2 ì´í•˜
    # bedrock_llm = Bedrock(
    #     model_id=model_version_id,
    #     client=bedrock_client,
    #     model_kwargs={'temperature': 0}
    #     )
    # bedrock_llm = BedrockChat(model_id=model_version_id, model_kwargs={'temperature': 0}, streaming=True)

    bedrock_llm = BedrockChat(
        model_id=bedrock_model_id, 
        model_kwargs={'temperature': 0},
        streaming=True,
        callbacks=[StreamHandler()]
        )
    return bedrock_llm


def get_bedrock_client():
    bedrock_client = boto3.client(
        "bedrock-runtime", region_name=bedrock_region)
    return bedrock_client


def create_vector_embedding_with_bedrock(text, bedrock_client):
    payload = {"inputText": f"{text}"}
    body = json.dumps(payload)
    modelId = "amazon.titan-embed-text-v1"
    accept = "application/json"
    contentType = "application/json"

    response = bedrock_client.invoke_model(
        body=body, modelId=modelId, accept=accept, contentType=contentType
    )
    response_body = json.loads(response.get("body").read())

    embedding = response_body.get("embedding")
    return {"_index": index_name, "text": text, "vector_field": embedding}

def create_opensearch_index(opensearch_client) :
    body = {
        'settings': {
            'index': {
                'number_of_shards': 3,
                'number_of_replicas': 2,
                "knn": True,
                "knn.space_type": "cosinesimil"
            }
        }
    }
    success = opensearch_client.indices.create(index_name, body=body)
    if success:
        body = {
            "properties": {
                "vector_field": {
                    "type": "knn_vector",
                    "dimension": 1536
                },
                "text": {
                    "type": "keyword"
                }
            }
        }
        success = opensearch_client.indices.put_mapping(
            index=index_name,
            body=body
        )


def extract_sentences_from_pdf(opensearch_client, pdf_file, progress_bar, progress_text):
    try:
        logging.info(
            f"Checking if index {index_name} exists in OpenSearch cluster")

        exists = opensearch_client.indices.exists(index=index_name)

        if not exists:
            create_opensearch_index(opensearch_client)

        doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
        all_records = []
        for page in doc:
            all_records.append(page.get_text())

        logging.info(f"PDF LIST ê°œìˆ˜ : {len(all_records)}")

        total_records = len(all_records)
        processed_records = 0

        bedrock_client = get_bedrock_client()

        all_json_records = []

        for record in all_records:
            if record_stop_yn and processed_records > stop_record_count:

                success, failed = bulk(opensearch_client, all_json_records)
                break

            records_with_embedding = create_vector_embedding_with_bedrock(
                record, bedrock_client)
            all_json_records.append(records_with_embedding)

            processed_records += 1
            progress = int((processed_records / total_records) * 100)
            progress_bar.progress(progress)

            if processed_records % 500 == 0 or processed_records == len(all_records):

                success, failed = bulk(opensearch_client, all_json_records)
                all_json_records = []

        progress_text.text("ì™„ë£Œ")
        logging.info("ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì½”ë“œ ìƒì„± ì™„ë£Œ")

        return total_records
    except Exception as e:
        print(str(e))
        st.error('PDFë¥¼ ì„ë² ë”© í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒë˜ì—ˆìŠµë‹ˆë‹¤.')
        return 0

def analytics_in_data(question, data):
    question = question
    bedrock_client = get_bedrock_client()
    bedrock_llm = create_bedrock_llm()

    bedrock_embeddings_client = create_langchain_vector_embedding_using_bedrock(
        bedrock_client)

    opensearch_vector_search_client = create_opensearch_vector_search_client(
        bedrock_embeddings_client)
    
    prompt_template = """
    Use the following pieces of context to answer the question at the end.
    ì§ˆë¬¸ì„ í†µí•´ ì–»ì€ ë°ì´í„°ì´ë‹¤. 
    ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì¤˜. 
    ë°ì´í„°ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œí•˜ê³  1ìœ„ ìƒí’ˆì˜ ì¤‘ìš” ìˆ«ìëŠ” (ì£¼ë¬¸ì „í™˜ìœ¨ ë“±) ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œí•´ì¤˜.
    ì¸ì‚¬ì´íŠ¸ ë‹¨ì–´ëŠ” ì´ˆë¡ìƒ‰ìœ¼ë¡œ í‘œì‹œí•˜ê³  ì¸ì‚¬ì´íŠ¸ ë‹¨ì–´ ì•ì— :star2: ë¥¼ ì ì–´ì¤˜.
    ë°ì´í„°ì˜ ì˜ë¯¸ ë¶€ë¶„ì—ì„œ ìˆ«ìëŠ” ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œí•´ì¤˜. ìƒ‰ê¹”ì„ ìœ„í•œ íƒœê·¸ëŠ” span ì„ ì‚¬ìš©í•´.
    ê¸ˆì•¡ì€ 1000ìë¦¬ ë§ˆë‹¤ ì½¤ë§ˆë¥¼ í‘œì‹œí•´ì¤˜.    
    ê³¼ê±° ëŒ€í™” ì´ë ¥ì˜ ë°ì´í„°ì™€ ì°¨ì´ê°€ ìˆë‹¤ë©´ ì›ì¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.
    ì¡°íšŒìˆ˜ëŠ” ë§ì§€ë§Œ ì£¼ë¬¸ì „í™˜ìœ¨ì´ ë‚®ì€ ìƒí’ˆì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.
    context ì •ë³´ëŠ” ë¬´ì‹œí•´.
    {context}
    
    # ì§ˆë¬¸
    {question}

    ë‹¤ìŒ ëŒ€í™”ì™€ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜.
    %s

    # ë°ì´í„°
    %s

    
    # Answer
    ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.(í…Œì´ë¸” í˜•íƒœì˜ ë°ì´í„°, í…Œì´ë¸” ì»¬ëŸ¼ì€ í•œê¸€ì„ ì‚¬ìš©)
    ...

    ë°ì´í„°ì˜ ì˜ë¯¸ì™€ ì–»ì„ ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 
    - í˜„ì¬ ë°ì´í„°
    - ê³¼ê±° ë°ì´í„°ì™€ì˜ ë¹„êµ, 1ìœ„ ìˆœìœ„ê°€ ë³€ê²½ë˜ì—ˆë‹¤ë©´ ê·¸ ìƒí’ˆëª…ì„ ì•Œë ¤ì¤˜.
    """

    data = data.replace("{","")
    data = data.replace("}","")

    memory, chat_history = get_memory_from_dynamo(identity_id)
    
    prompt_template = PromptTemplate(
            template=prompt_template % (chat_history, data), 
            input_variables=["context", "chat_history", "question"]
        )

    qa = RetrievalQA.from_chain_type(llm=bedrock_llm,
                                        chain_type="stuff",
                                        retriever=opensearch_vector_search_client.as_retriever(),
                                        # return_source_documents=True,
                                        return_source_documents=False,
                                        memory= memory,
                                        chain_type_kwargs={
                                            "prompt": prompt_template})



    response = qa(question,
                      return_only_outputs=False)
    
    return f"{response.get('result')}"


def find_answer_in_sentences(select_option, question):
    # try:
    print("# select_option : ", select_option)
    question = question
    bedrock_client = get_bedrock_client()
    bedrock_llm = create_bedrock_llm()

    bedrock_embeddings_client = create_langchain_vector_embedding_using_bedrock(
        bedrock_client)

    opensearch_vector_search_client = create_opensearch_vector_search_client(
        bedrock_embeddings_client)
    
    
    prompt_template = {
        0 : """
            ë„ˆëŠ” AíšŒì‚¬ì—ì„œ ê·¼ë¬´í•˜ëŠ” ì§ì›ë“¤ì—ê²Œ ë‚´ë¶€ ìš©ì–´, ê³„ì‚°ì‹, ê·¸ë¦¬ê³  ë°ì´í„° ì¡°íšŒë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì´ë‹¤. 
            contextì— ì œê³µëœ ë‚´ìš©ì´ ì—†ë‹¤ë©´ ë°˜ë“œì‹œ ì•Œìˆ˜ì—†ë‹¤ê³  ëŒ€ë‹µí•´ì¤˜.
            contextì— ì œê³µëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì§ˆë¬¸í•œ ê²ƒì— ëŒ€í•œ ì •ì˜ì™€ ê³„ì‚°ì‹ ê·¸ë¦¬ê³  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ë¥¼ markdown ì„ ì‚¬ìš©í•´ì„œ í–‰ ì¤‘ì‹¬ì˜ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ëŒ€ë‹µí•´ì¤˜. SQLì€ ì‘ì„±í•˜ì§€ë§ˆ.
            ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ëŠ” ë¶‰ì€ìƒ‰ìœ¼ë¡œ í‘œì‹œí•´ì¤˜.
            {context}

            ë‹¤ìŒ ëŒ€í™”ì™€ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜.
            %s


            * Question: {question}
            * Answer:
            """,
        1 : """
            ë„ˆëŠ” AíšŒì‚¬ì—ì„œ ê·¼ë¬´í•˜ëŠ” ì§ì›ë“¤ì—ê²Œ ë‚´ë¶€ ìš©ì–´, ê³„ì‚°ì‹, ê·¸ë¦¬ê³  ë°ì´í„° ì¡°íšŒë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì´ë‹¤. 
            ë°ì´í„° ìš”ì²­ì„ í•˜ë©´ contextì— ì •í™•í•œ ë°ì´í„°ë² ì´ìŠ¤ëª…, í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…ì´ ì—†ë‹¤ë©´ ì˜ˆì¸¡í•´ì„œ SQLì„ ì‘ì„±í•˜ì§€ ë§ê³  í…Œì´ë¸”ëª…ì„ ì•Œë ¤ë‹¬ë¼ê³  ë§í•´ì¤˜.
            ë°ì´í„° ìš”ì²­ì„ í•˜ë©´ contextì— ì •í™•í•œ í…Œì´ë¸”ëª…ê³¼ ì»¬ëŸ¼ëª…ì´ ëª¨ë‘ ìˆì„ ê²½ìš°ì—ë§Œ AWS ATHENAì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ SQL ì„ MARKDOWN ì½”ë“œì˜ SQLíƒœê·¸ ì•ˆì— ì‘ì„±í•´ì¤˜, SQL ì€ <details open><summary>SQLì œëª©</summary>```sql\nSQL```</details> í¬ë§·ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
            {context}


            * Question: {question}
            * Answer:
            """
    }
        
    print("## prompt : ", prompt_template[select_option])
    

    prompt = PromptTemplate(
        template=prompt_template[select_option], input_variables=["context", "question"]
    )


    qa = RetrievalQA.from_chain_type(llm=bedrock_llm,
                                        chain_type="stuff",
                                        retriever=opensearch_vector_search_client.as_retriever(),
                                        # return_source_documents=True,
                                        return_source_documents=False,
                                        chain_type_kwargs={
                                            "prompt": prompt})
    
    response = qa(question,
                    return_only_outputs=False)

    return f"{response.get('result')}"

def connect_to_database():
    return pymysql.connect(
        host=mysql_host,
        port=int(mysql_port),
        user=mysql_user,
        password=mysql_password,
        database=mysql_db,
        charset='utf8mb4'
    )

def execute_query_and_return_df(sql):
    conn = connect_to_database()
    try:
        with conn.cursor() as cursor:
            cursor.execute(sql)
            result = cursor.fetchall()
            df = pd.DataFrame(result, columns=[i[0]
                              for i in cursor.description])
    finally:
        conn.close()
    return df


def execute_query_athena(client, sql):
    
    s3_client = boto3.client('s3', region_name='ap-northeast-2')
    
    s3_output = 's3://korea-summit2024/tmp/athena'
    response = client.start_query_execution(
        QueryString=sql,
        ResultConfiguration={
            'OutputLocation': s3_output,
        }
    )
    query_execution_id = response['QueryExecutionId']

    while True:
        query_status = client.get_query_execution(QueryExecutionId=query_execution_id)
        query_execution_status = query_status['QueryExecution']['Status']['State']
        
        if query_execution_status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
            break
        else:
            time.sleep(2)
    
    if query_execution_status == 'SUCCEEDED':
        result_location = query_status['QueryExecution']['ResultConfiguration']['OutputLocation']
        filename = result_location.split('/')[-1]
        s3_path = f's3://{s3_output}/{filename}'
        
        df = pd.read_csv(s3_path)
        json_str = df.to_json(orient='records', lines=True)

        return json_str

    



def main():
    

    
    opensearch_client = get_opensearch_cluster_client()
    st.set_page_config(page_title='ğŸ¤– AWS Summit 2024', layout='wide')
    st.header(':blue[ë°ì´í„°ê°€] _ê¶ê¸ˆí•´ ?!_', divider='rainbow')    

    if 'qa_history' not in st.session_state:
        st.session_state.qa_history = []

    with st.sidebar:
        
        selected_option = st.sidebar.selectbox(
            'ì–´ë–¤ ì˜µì…˜ì„ ì„ íƒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?',
            select_options
        )
        selected_index = select_options.index(selected_option)
        st.session_state.select_option = selected_index
        st.sidebar.markdown('---')
        st.title("ğŸ” RAG Embedding")
        pdf_file = st.file_uploader(
            "PDF ì—…ë¡œë“œë¥¼ í†µí•´ ì¶”ê°€ í•™ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", type=["pdf"], key=None)
        

        if 'last_uploaded' not in st.session_state:
            st.session_state.last_uploaded = None

        if pdf_file is not None and pdf_file != st.session_state.last_uploaded:

            progress_text = st.empty()
            st.session_state['progress_bar'] = st.progress(0)
            progress_text.text("RAG(OpenSearch) ì„ë² ë”© ì¤‘...")
            record_cnt = extract_sentences_from_pdf(
                opensearch_client, pdf_file, st.session_state['progress_bar'], progress_text)
            if record_cnt > 0:
                st.session_state['processed'] = True
                st.session_state['record_cnt'] = record_cnt
                st.session_state['progress_bar'].progress(100)
                st.session_state.last_uploaded = pdf_file
                st.success(f"{record_cnt} Vector ì„ë² ë”© ì™„ë£Œ!")
        
        if st.button("ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ"):
            exists = opensearch_client.indices.exists(index=index_name)

            if exists:
                opensearch_client.indices.delete(index=index_name)
                create_opensearch_index(opensearch_client)            
                logging.info("OpenSearch index successfully deleted")
                st.success("OpenSearch ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œ/ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else :
                create_opensearch_index(opensearch_client)            
                st.success("OpenSearch ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        st.sidebar.markdown('---')

    if "messages" not in st.session_state.keys():
        st.session_state.messages = [INIT_MESSAGE]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["type"] == "text":
                st.markdown(message["content"], unsafe_allow_html=True)

    question = st.chat_input("Say something")

    if question:
        st.session_state.messages.append({"role": "user",
                                          "type": "text",
                                          "content": question})
        with st.chat_message("user"):
            st.markdown(question, unsafe_allow_html=True)

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                answer = find_answer_in_sentences(st.session_state.select_option, question)

            with st.spinner("ë°ì´í„° ì¡°íšŒ ì¤‘..."):

                try :
                    sql_queries = re.findall(r'```sql(.*?)```', answer, re.DOTALL)

                    if len(sql_queries) > 0:
                        sql = sql_queries[0]
                        df = execute_query_athena(get_athena_client(), sql)
                        analytics_in_data(question, df)
                except Exception as e:                    
                    print("# ì—ëŸ¬", str(e))
                    pass


if __name__ == "__main__":
    main()
